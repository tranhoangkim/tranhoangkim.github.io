<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Kim Tran</title>

    <meta name="author" content="Kim Tran">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:1.5%;width:72%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Kim Tran
                </p>
                <p>
    I am currently a Master's student in <a href="https://engineering.uark.edu/electrical-engineering-computer-science/">Computer Science at the University of Arkansas</a>, advised by <a href="https://engineering.uark.edu/electrical-engineering-computer-science/electrical-engineering-faculty/uid/jgauch/name/John+Michael+Gauch/">Prof. John Gauch</a> and <a href="https://scholar.google.com/citations?user=8ck0k_UAAAAJ&hl=en">Prof. Ngan Le</a> in the areas of computer vision and computer graphics. Prior to my master's, I was an AI Research Resident at <a href="https://fpt.com/en/news/fpt-news/fpt-software-ai-center">FPT AI Center</a>, where I had the privilege of working closely with Prof. Ngan Le. I earned my bachelor's degree in Computer Science from <a href="https://en.hcmus.edu.vn">National University, University of Science, Vietnam</a>.<br>
    In the summer of 2025, I interned at <a href="https://www.linkedin.com/company/koidra/posts/?feedView=all">Koidra</a>, Seattle, as a Physics-Informed Machine Learning Researcher, where I worked directly with <a href="https://www.linkedin.com/in/ktran/">Dr. Kenneth Tran</a>, the CTO of Koidra and former Principal Research Scientist at Microsoft — to develop digital twins for greenhouses that simulate air temperature, vapor pressure, and energy consumption.
                </p>
                <p style="text-align:center">
                  <a href="mailto:tranhoangkim981@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://drive.google.com/file/d/16uGqMiUSnbhc2I80ePz3e1CvyFp8jHEs/view?usp=sharing">Resume</a> &nbsp;/&nbsp;
                  <a href="https://leetcode.com/u/tranhoangkim981/">Leetcode</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=SQXHftAAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/kim-tran-766363244/">Linkedin</a> &nbsp;/&nbsp;
                  <a href="https://github.com/tranhoangkim?tab=repositories">Github</a>
                </p>
              </td>
              <td style="padding:1.5%;width:28%;max-width:28%">
                <a href="images/kimtran.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/kimtran.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p style="margin-bottom: 20px;">
                 My motivation for doctoral research stems from a compelling challenge: while reinforcement learning has achieved remarkable successes in simulated domains (e.g. games and robotics simulators), <strong>most real-world control problems cannot rely on unlimited data or perfect simulators</strong>. Motivated by this, my research interests lie at the intersection of model-based reinforcement learning, physics-informed modeling, graph neural networks, and control systems, with a focus on enabling intelligent agents to operate safely and efficiently in real-world environments with <strong>limited data</strong>. I am curious about three key questions:
                </p>
                <p style="margin-bottom: 12px; margin-left: 20px;">
                  • How can reinforcement learning be applied across diverse practical domains such as industrial automation, education, healthcare, and aerospace—beyond the current emphasis on LLMs and robotics?
                </p>
                <p style="margin-bottom: 20px; margin-left: 20px;">
                  • How can multi-modality (drawing from my background in vision-language models and 3D geometry) be effectively integrated into reinforcement learning to enhance perception and decision-making?
                </p>
                <p style="margin-bottom: 20px; margin-left: 20px;">
                  • How can we bridge the gap between simulation and reality in real-world control problems with limited data?
                </p>
                <p style="margin-bottom: 20px;">
                  My long-term goal through a PhD program, is to create <strong>adaptive, interpretable, and generalizable reinforcement learning frameworks </strong> that can be applied to complex and practical domains and advance the foundation of learning-based control and its integration with physics-based models, perception and graph-structured representations for real-world decision-making.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:6px;"><tbody>
            <tr>
              <td>
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 20px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr onmouseout="ever_stop()" onmouseover="ever_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='ever_image'>
					  <img src='images/tracking_before.png' width=100%>
					</div>
          <img src='images/tracking_after.png' width=100%>
        </div>
        <script type="text/javascript">
          function ever_start() {
            document.getElementById('ever_image').style.opacity = "1";
          }

          function ever_stop() {
            document.getElementById('ever_image').style.opacity = "0";
          }
          ever_stop()
        </script>
      </td>
      <td style="padding:12px;width:80%;vertical-align:middle">
        <a href="https://fsoft-aic.github.io/Z-GMOT/">
			<span class="papertitle">Z-GMOT: Zero-shot Generic Multiple Object Tracking
</span>
        </a>
        <br>
        <p style="margin: 0; line-height: 1.3;"><strong>Kim Tran</strong>, Anh Duy Le Dinh, Tien-Phat Nguyen, Thinh Phan, Pha Nguyen, Khoa Luu, Donald Adjeroh, Gianfranco Doretto, Ngan Le</p>
				<br style="line-height: 1.2;">
        <em style="line-height: 1.3;">Findings of NAACL</em>, 2024 &nbsp
        <br style="line-height: 1.2;">
        <a href="https://fsoft-aic.github.io/Z-GMOT/">Project Page</a>
        /
        <a href="https://aclanthology.org/2024.findings-naacl.220/">Proceedings</a>
        <p style="margin: 0; line-height: 1.3;">
          With the insight that existing vision-language models rely heavily on object categories while overlooking object attributes and characteristics, we proposed a benchmark to evaluate this limitation and introduced a framework for generic multiple-object tracking from natural language descriptions, 
          enabling models to track unseen categories by focusing on object attributes without requiring retraining.
        </p>
      </td>
    </tr>

    <tr onmouseout="ever_stop()" onmouseover="ever_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='ever_image'>
					  <img src='images/model_before.png' width=100%>
					</div>
          <img src='images/model_after.png' width=100%>
        </div>
        <script type="text/javascript">
          function ever_start() {
            document.getElementById('ever_image').style.opacity = "1";
          }

          function ever_stop() {
            document.getElementById('ever_image').style.opacity = "0";
          }
          ever_stop()
        </script>
      </td>
      <td style="padding:12px;width:80%;vertical-align:middle">
        <a href="https://fsoft-aic.github.io/UGLF/">
			<span class="papertitle">Unifying Global and Local Scene Entities Modelling for Precise Action Spotting
</span>
        </a>
        <br>
        <p style="margin: 0; line-height: 1.3;"><strong>Kim Tran<sup>*</sup></strong>, Phuc Do<sup>*</sup> , Ngoc Ly, Ngan Le</p>
				<br style="line-height: 1.2;">
        <em style="line-height: 1.3;">IJCNN</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
        <br style="line-height: 1.2;">
        <a href="https://fsoft-aic.github.io/UGLF/">Project Page</a>
        /
        <a href="https://arxiv.org/pdf/2404.09951">Arxiv</a>
        /
        <a href="https://ieeexplore.ieee.org/document/10650009">Proceedings</a>
        <p style="margin: 0; line-height: 1.3;">
          Motivated by the fact that existing video action recognition models process entire frames without adequately considering the objects in the scene and their interactions, our method jointly modeled
          global scene context and local object interactions (e.g., players, goal, ball, and referee in soccer), achieving state-of-the-art performance (Ranked #1 on the SoccerNet 2024 leaderboard).
        </p>
      </td>
    </tr>

    <tr onmouseout="ever_stop()" onmouseover="ever_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='ever_image'>
					  <img src='images/multiview_after.png' width=100%>
					</div>
          <img src='images/multiview_before.png' width=100%>
        </div>
        <script type="text/javascript">
          function ever_start() {
            document.getElementById('ever_image').style.opacity = "1";
          }

          function ever_stop() {
            document.getElementById('ever_image').style.opacity = "0";
          }
          ever_stop()
        </script>
      </td>
      <td style="padding:12px;width:80%;vertical-align:middle">
        <a href="https://www.sciencedirect.com/science/article/pii/S277237552500543X/pdfft?md5=19a398460ad0d8a56a93ea9de8687952&pid=1-s2.0-S277237552500543X-main.pdf">
			<span class="papertitle">BroilerTrack: Automatic multi-camera multi-broiler tracking
</span>
        </a>
        <br>
        <p style="margin: 0; line-height: 1.3;">Thinh Phan, <strong>Kim Tran</strong>, Andrew Lockett, Isaac Phillips, Hao Vo, Duy Le, Michael T. Kidd, James Mason, Santiago Avendano, Ngan Le</p>
				<br style="line-height: 1.2;">
        <em style="line-height: 1.3;">Smart Agricultural Technology</em>, 2025 &nbsp
        <br style="line-height: 1.2;">
        <a href="https://www.sciencedirect.com/science/article/pii/S277237552500543X/pdfft?md5=19a398460ad0d8a56a93ea9de8687952&pid=1-s2.0-S277237552500543X-main.pdf">Proceedings</a>
        <p style="margin: 0; line-height: 1.3;">
          Developed a multi-camera tracking system for tracking poultry in real-world farm environments. By performing camera
          calibration and designing a multi-view tracking algorithm, the system can accurately track at least 10 chickens across
          six views of each pen with very high consistency (each chicken is assigned a unique ID that is maintained over time and
          across all six views).
        </p>
      </td>
    </tr>

    <tr onmouseout="ever_stop()" onmouseover="ever_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='ever_image'>
					  <img src='images/tp-gmot_before.png' width=100%>
					</div>
          <img src='images/tp-gmot_after.png' width=100%>
        </div>
        <script type="text/javascript">
          function ever_start() {
            document.getElementById('ever_image').style.opacity = "1";
          }

          function ever_stop() {
            document.getElementById('ever_image').style.opacity = "0";
          }
          ever_stop()
        </script>
      </td>
      <td style="padding:12px;width:80%;vertical-align:middle">
        <a href="https://ebooks.iospress.nl/volumearticle/69595?_gl=1*163jrhi*_up*MQ..*_ga*MTM5MTEzNzY2MS4xNzQ0MTM0MjA1*_ga_6N3Q0141SM*MTc0NDEzNDIwNS4xLjEuMTc0NDEzNDIyNy4wLjAuMA..">
			<span class="papertitle">TP-GMOT: Tracking Generic Multiple Object by Textual Prompt with Motion-Appearance Cost (MAC) SORT
</span>
        </a>
        <br>
        <p style="margin: 0; line-height: 1.3;">Duy Le Dinh Anh, <strong>Kim Tran</strong>, Ngan Le</p>
				<br style="line-height: 1.2;">
        <em style="line-height: 1.3;">ECAI</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
        <br style="line-height: 1.2;">
        <a href="https://ebooks.iospress.nl/volumearticle/69595?_gl=1*163jrhi*_up*MQ..*_ga*MTM5MTEzNzY2MS4xNzQ0MTM0MjA1*_ga_6N3Q0141SM*MTc0NDEzNDIwNS4xLjEuMTc0NDEzNDIyNy4wLjAuMA..">Proceedings</a>

      </td>
    </tr>

    <tr onmouseout="ever_stop()" onmouseover="ever_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='ever_image'>
					  <img src='images/accv_before.png' width=100%>
					</div>
          <img src='images/accv_after.png' width=100%>
        </div>
        <script type="text/javascript">
          function ever_start() {
            document.getElementById('ever_image').style.opacity = "1";
          }

          function ever_stop() {
            document.getElementById('ever_image').style.opacity = "0";
          }
          ever_stop()
        </script>
      </td>
      <td style="padding:12px;width:80%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/ACCV2024/papers/Le_Dinh_Anh_Enhanced_Kalman_with_Adaptive_Appearance_Motion_SORT_for_Grounded_Generic_ACCV_2024_paper.pdf">
			<span class="papertitle">Enhanced Kalman with Adaptive Appearance Motion SORT for Grounded Generic Multiple Object Tracking
</span>
        </a>
        <br>
        <p style="margin: 0; line-height: 1.3;">Duy Le Dinh Anh, <strong>Kim Tran</strong>, Quang Thuc Nguyen, Ngan Le</p>
				<br style="line-height: 1.2;">
        <em style="line-height: 1.3;">ACCV</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
        <br style="line-height: 1.2;">
        <a href="https://openaccess.thecvf.com/content/ACCV2024/papers/Le_Dinh_Anh_Enhanced_Kalman_with_Adaptive_Appearance_Motion_SORT_for_Grounded_Generic_ACCV_2024_paper.pdf">Proceedings</a>

      </td>
    </tr>

    </table>
  </body>
</html>
